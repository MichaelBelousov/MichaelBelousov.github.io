{"componentChunkName":"component---src-components-blog-page-tsx","path":"/blog/latex_and_python/","result":{"data":{"markdownRemark":{"html":"<p>The one nagging gap in my homework development stack early on\nwas an itch for\nprogrammability, that trait which Excel is alleged to have.\nBut I have a gripe with Excel. It's like Scratch for adults,\nA psuedoscripting language for the uninitiated and unlearned.\nPersonally, I've always kicked up a Python interpreter any\ntime I needed a calculation, and I quickly began depending on\nPython+scipy for more intense derivations and data routines. I scratched\nup a utility module for producing LaT<sub>E</sub>X tables and pgfplots graphics\nfrom various data formats, and used it frequently with minimal\ncopy and paste glue (thank you Cygwin's\n<code class=\"language-latex\">/dev/clipboard</code>).</p>\n<p>But larger projects begged for more complete tools; like that assignment\ninvolving the visualization\nof 26 different samples from a dataset, as well as calculating some\nstatistical metrics about each sample.\nA sane person would have used the charting features\nin the Excel spreadsheet it was provided in, but that's outside the scope\nof this post.\nI exported to csv immediately and made an incredibly primitive\ntoolkit for embedding Python logic in LaT<sub>E</sub>X. Revisiting it a\ncouple of months later, I refined my toolkit massively and today I\nwould consider the design complete, but alternative implementation\napproaches merit some exploration.</p>\n<p>The first time I attemped glueing my Pythonisms into LaT<sub>E</sub>X,\nmy design was a tiny mapping from a LaT<sub>E</sub>X grammar extension\nto Python formatting key syntax.\nIf I were to write my LaT<sub>E</sub>X code directly in a Python string,\nand attempt to use formatting keys,\nI would probably spontaneously combust. It looks like this:</p>\n<div class=\"gatsby-highlight\" data-language=\"latex\"><pre class=\"language-latex\"><code class=\"language-latex\">r\"\"\"<span class=\"token function selector\">\\begin</span><span class=\"token punctuation\">{</span><span class=\"token keyword\">{document</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">}</span>\n<span class=\"token function selector\">\\frac</span><span class=\"token punctuation\">{</span><span class=\"token punctuation\">{</span> <span class=\"token punctuation\">{</span>kill<span class=\"token punctuation\">}</span> <span class=\"token punctuation\">}</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">{</span><span class=\"token punctuation\">{</span> <span class=\"token punctuation\">{</span>me<span class=\"token punctuation\">}</span> <span class=\"token punctuation\">}</span><span class=\"token punctuation\">}</span>\n<span class=\"token function selector\">\\end</span><span class=\"token punctuation\">{</span><span class=\"token keyword\">{document</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">}</span>\"\"\".format(kill=5, me=10)</code></pre></div>\n<p>That's almost as crufty as chained ternary expressions in JSX.\nBackslash doubling I can avoid by making it a raw string, but that\nisn't an option in all use cases.\nRegardless, I hid this formatting ugliness behind a script, which\nescaped braces for me, and interpretted the characters '&#x3C;&#x3C;' as '{', and\n'>>' as '}'. So now I didn't have to brace every 5 characters, and the\nkeys looked distinct from the LaT<sub>E</sub>X syntax.\nMy dumb and ugly LaT<sub>E</sub>X superset templater took about 5 lines of\nPython code to process, and the source looked like:</p>\n<div class=\"gatsby-highlight\" data-language=\"latex\"><pre class=\"language-latex\"><code class=\"language-latex\"><span class=\"token function selector\">\\begin</span><span class=\"token punctuation\">{</span><span class=\"token keyword\">document</span><span class=\"token punctuation\">}</span>\n<span class=\"token function selector\">\\frac</span><span class=\"token punctuation\">{</span>&lt;&lt;numerator>><span class=\"token punctuation\">}</span><span class=\"token punctuation\">{</span>&lt;&lt;denominator>><span class=\"token punctuation\">}</span>\n<span class=\"token function selector\">\\end</span><span class=\"token punctuation\">{</span><span class=\"token keyword\">document</span><span class=\"token punctuation\">}</span></code></pre></div>\n<p>Quite literally, all my template processor had to do was replace all braces with\nescaped double braces, and then replace all\ndouble angle brackets with a single formatting brace, thus yielding\nthe valid Python formatting keys, while allowing raw LaT<sub>E</sub>X to look\nat least familiar.\nThen you just produce your values and/or data in your Python script,\nread this template from some file,\nand submit it to Python's formatting functinos with the expected keyword\narguments that you used in the document source\n(e.g. &#x3C;<numerator>>).\nHere's the naive templating implementation:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">load_pytex</span><span class=\"token punctuation\">(</span>path<span class=\"token punctuation\">,</span> <span class=\"token operator\">**</span>kwargs<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  txt <span class=\"token operator\">=</span> <span class=\"token builtin\">open</span><span class=\"token punctuation\">(</span>path<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>read<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n  txt <span class=\"token operator\">=</span> txt<span class=\"token punctuation\">.</span>replace<span class=\"token punctuation\">(</span><span class=\"token string\">'{'</span><span class=\"token punctuation\">,</span><span class=\"token string\">'{{'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>replace<span class=\"token punctuation\">(</span><span class=\"token string\">'}'</span><span class=\"token punctuation\">,</span><span class=\"token string\">'}}'</span><span class=\"token punctuation\">)</span>    \n  txt <span class=\"token operator\">=</span> txt<span class=\"token punctuation\">.</span>replace<span class=\"token punctuation\">(</span><span class=\"token string\">'&lt;&lt;'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'{'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>replace<span class=\"token punctuation\">(</span><span class=\"token string\">'>>'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'}'</span><span class=\"token punctuation\">)</span>\n  txt<span class=\"token punctuation\">.</span><span class=\"token builtin\">format</span><span class=\"token punctuation\">(</span><span class=\"token operator\">**</span>kwargs<span class=\"token punctuation\">)</span>\n  <span class=\"token keyword\">return</span> txt  <span class=\"token comment\"># or subproc.check_output(['pdflatex' ...])</span></code></pre></div>\n<p>So this works, but it would be quite annoying to have a Python\ncontext so decoupled from the usage context. Calculating things\nwithout regard to the location in the document, and being\nunable to change the logic while editing the document without\nswitching files.\nCertainly I could do better? So I did something sinful.</p>\n<p>Fake macros in LaT<sub>E</sub>X will make this all look way more T<sub>E</sub>X-savvy.\nLet's add a <code class=\"language-latex\"><span class=\"token function selector\">\\pyeval</span></code> pseudo-command, and a\n<code class=\"language-text\">pyexec</code> pseudo-environment so that we can get something\nlike so:</p>\n<div class=\"gatsby-highlight\" data-language=\"latex\"><pre class=\"language-latex\"><code class=\"language-latex\"><span class=\"token function selector\">\\begin</span><span class=\"token punctuation\">{</span><span class=\"token keyword\">pyexec</span><span class=\"token punctuation\">}</span>\nname = 'John'\n<span class=\"token function selector\">\\end</span><span class=\"token punctuation\">{</span><span class=\"token keyword\">pyexec</span><span class=\"token punctuation\">}</span>\n\nHello I am <span class=\"token function selector\">\\pyeval</span><span class=\"token punctuation\">{</span>' '.join(<span class=\"token punctuation\">[</span>name,'Smith'<span class=\"token punctuation\">]</span>)<span class=\"token punctuation\">}</span></code></pre></div>\n<p>Originally I implemented (knowing my transgressions) a naive\nregular expression based implementation, but this has many problems</p>\n<ul>\n<li>I want to ignore false ends in quotes and comments</li>\n<li>I want to allow counting of nested braces in pyeval</li>\n</ul>\n<p>expressions so that you don't need to escape all right\nbraces. (Regular expressions can't find the valid ending\nbrace of a nested expression)\nCheckout the following counter_examples:</p>\n<div class=\"gatsby-highlight\" data-language=\"latex\"><pre class=\"language-latex\"><code class=\"language-latex\"><span class=\"token function selector\">\\begin</span><span class=\"token punctuation\">{</span><span class=\"token keyword\">pyexec</span><span class=\"token punctuation\">}</span>\nhate_you = r'<span class=\"token function selector\">\\end</span><span class=\"token punctuation\">{</span><span class=\"token keyword\">pyexec</span><span class=\"token punctuation\">}</span>'  # we need to avoid ending if its in a literal\n<span class=\"token function selector\">\\end</span><span class=\"token punctuation\">{</span><span class=\"token keyword\">pyexec</span><span class=\"token punctuation\">}</span>\n\n<span class=\"token function selector\">\\pyeval</span><span class=\"token punctuation\">{</span>h = <span class=\"token punctuation\">{</span>'hello': 'world', 'x': <span class=\"token punctuation\">{</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">}</span>  # which brace do we end on?</code></pre></div>\n<p>So I couldn't use regular expressions for this, but I still wanted it.\nBecause my LaT<sub>E</sub>X document source would look like this:</p>\n<div class=\"gatsby-highlight\" data-language=\"latex\"><pre class=\"language-latex\"><code class=\"language-latex\"><span class=\"token function selector\">\\begin</span><span class=\"token punctuation\">{</span><span class=\"token keyword\">pyexec</span><span class=\"token punctuation\">}</span>\nfrom scipy.optimize import curve_fit\nfrom random import random as rand\nfrom itertools import count, product as crossproduct\nbin = crossproduct((0,1),repeat=5)\ndata = list(zip(count(), bin))\n# add a count of set bits as a column\nfor row in data:\nd, b = row\nrow.append(b.count(1))\nrandpts = <span class=\"token punctuation\">[</span><span class=\"token punctuation\">[</span>rand(),rand()<span class=\"token punctuation\">]</span> for _ in range(100)<span class=\"token punctuation\">]</span>\n<span class=\"token function selector\">\\end</span><span class=\"token punctuation\">{</span><span class=\"token keyword\">pyexec</span><span class=\"token punctuation\">}</span>\n\n<span class=\"token function selector\">\\section*</span><span class=\"token punctuation\">{</span><span class=\"token headline class-name\">Data Stuff</span><span class=\"token punctuation\">}</span>\n\n<span class=\"token function selector\">\\begin</span><span class=\"token punctuation\">{</span><span class=\"token keyword\">tabular</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">{</span>|c|c|<span class=\"token punctuation\">}</span>\n<span class=\"token function selector\">\\hline</span>\nDecimal <span class=\"token punctuation\">&amp;</span> Binary <span class=\"token punctuation\">&amp;</span> 1-Bits <span class=\"token function selector\">\\\\</span>\n<span class=\"token function selector\">\\hline</span>\n<span class=\"token function selector\">\\pyeval</span><span class=\"token punctuation\">{</span>'<span class=\"token function selector\">\\n</span>'.join(('<span class=\"token punctuation\">&amp;</span>'.join(map(str,d)) for d in data))<span class=\"token punctuation\">}</span>\n<span class=\"token comment\">% we can extract the above one-liner for reuse in tabular </span>\n<span class=\"token comment\">% construction and any horizontally partitioned context</span>\n<span class=\"token function selector\">\\hline</span>\n<span class=\"token function selector\">\\end</span><span class=\"token punctuation\">{</span><span class=\"token keyword\">tabular</span><span class=\"token punctuation\">}</span>\n\n<span class=\"token comment\">% use unpack+zip hack to get separate lists of x then y coords</span>\nBest Fit: <span class=\"token function selector\">\\pyeval</span><span class=\"token punctuation\">{</span>curve_fit(lambda x,m,b: m*x+b, zip(*randpts))<span class=\"token punctuation\">}</span> </code></pre></div>\n<p>Mmmmm... delicious. It looks so much like Python yet so much like LaT<sub>E</sub>X.\nIt's both at once, without crippling either one! It's logic embedded in the document\nmarkup cleanly. So I did go and implement it. Originally I tried using a more\ncomplicated parsing approach, PyParsing, but it turned out to be far too complicated\nfor what was essentially templating. I ended up settling on parsing by character, due\nto the simplicity of the extension. There are a few performance enhancements I could\nmake but I'm not in the need of them right now, and I've noted them for posterity in\nmy README. This is PyLaT<sub>E</sub>X, and I put it on my\n<a href=\"https://github.com/MichaelBelousov/pylatexc\">GitHub</a></p>\n<p>At last, I would argue, that we have a tool more extensible than Excel,\nit's got all of PyPI behind it, with numpy, scipy, and all the packages\nyou could wantâ€” embedded in your document.\nIf you like this approach, maybe you can be just as sinful and throw it into\nMarkdown, or change up the scripting language to your preference. So long as you\ndon't use php. Even if it's already a templating language.</p>\n<div class=\"gatsby-highlight\" data-language=\"markdown\"><pre class=\"language-markdown\"><code class=\"language-markdown\"><span class=\"token title important\">My Document\n<span class=\"token punctuation\">===========</span></span>\n\n<span class=\"token code\"><span class=\"token punctuation\">```</span><span class=\"token code-language\">pyexec</span>\n<span class=\"token code-block language-pyexec\">import csv\ndata = list(csv.reader('mycsv.csv'))</span>\n<span class=\"token punctuation\">```</span></span>\n<span class=\"token code keyword\">`>>> '\\n'.join(data)`</span></code></pre></div>\n<p>And as a final note, check out <a href=\"https://typora.io/\">Typora</a>\nother cool ways to not use Office products. I don't actually use it, but the LaT<sub>E</sub>X\nin Markdown makes me want to. Even if I'm not in school any more.</p>\n<style>\nspan.vim {\n    font-size: 3pt;\n}\n</style>","frontmatter":{"title":"Empowering LaTeX (PyLaTeXc)","path":"/blog/empowering-LaTeX","date":"May 4, 2019"}}},"pageContext":{"slug":"/blog/latex_and_python/"}}}